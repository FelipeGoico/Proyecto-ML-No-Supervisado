import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import librosa
import librosa.display
from typing import Optional, Iterable
from IPython.display import display
from pathlib import Path


DEFAULT_WIDTH = 50


def show_section(title: str, emoji: Optional[str] = None, width: int = DEFAULT_WIDTH) -> None:
    """Imprime un t√≠tulo en MAY√öSCULAS con separador est√°ndar y un salto de l√≠nea.
    - title: texto del t√≠tulo (se convertir√° a MAY√öSCULAS)
    - emoji: emoji opcional que se antepone al t√≠tulo
    - width: largo del separador (por defecto 50)
    """
    line = title.upper()
    if emoji:
        line = f"{emoji} {line}"
    print(line)
    print("=" * width)
    print()


def show_subsection(text: str, emoji: Optional[str] = None) -> None:
    """Imprime un subt√≠tulo en MAY√öSCULAS con un salto de l√≠nea previo."""
    line = text.upper()
    if emoji:
        line = f"{emoji} {line}"
    print()
    print(line)


def show_list(items: Iterable, bullet: str = "-") -> None:
    """Imprime una lista simple con vi√±etas."""
    for it in items:
        print(f"{bullet} {it}")


def print_kv(key: str, value, bullet: str = "-") -> None:
    """Imprime una l√≠nea tipo clave: valor con vi√±eta opcional."""
    print(f"{bullet} {key}: {value}")


def blank(n: int = 1) -> None:
    """Imprime n saltos de l√≠nea."""
    for _ in range(n):
        print()


# ================================================================================
# FUNCIONES DE AN√ÅLISIS DE DATASETS ESPEC√çFICOS
# ================================================================================

def create_datasets_summary_table():
    """Crea y muestra la tabla resumen estilizada de los 10 datasets."""
    datasets_info = {
        "No.": list(range(1, 11)),
        "Dataset": [
            "Dairy Goods Sales Dataset", "Animal Sounds Dataset",
            "Customer Personality Analysis", "News Category Dataset",
            "Global Coffee Health Dataset", "Credit Card Customer",
            "Sleep Health and Lifestyle", "Wholesale Customers Dataset",
            "PlantVillage Dataset", "Milk Quality Dataset"
        ],
        "Dominio": [
            "Comercio/Ventas", "Audio/Sonidos", "Comportamiento del Consumidor",
            "Texto/NLP", "Salud Global", "Servicios Financieros",
            "Salud y Bienestar", "Comercio B2B", "Visi√≥n por Computador",
            "Calidad Alimentaria"
        ],
        "Tipo_de_Datos": [
            "Transaccional", "Audio (WAV)", "Demogr√°fico/Comportamental",
            "Texto/JSON", "Num√©rico/Geogr√°fico", "Financiero/Num√©rico",
            "M√©dico/Estilo de vida", "Ventas por categor√≠a",
            "Im√°genes RGB", "Fisicoqu√≠mico/Num√©rico"
        ],
        "Formato_Archivo": [
            "CSV", "WAV/Audio", "CSV", "JSON", "CSV",
            "CSV", "CSV", "CSV", "JPG/PNG", "CSV"
        ],
        "URL_Origen": [
            "https://www.kaggle.com/datasets/suraj520/dairy-goods-sales-dataset",
            "https://github.com/YashNita/Animal-Sound-Dataset",
            "https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis",
            "https://www.kaggle.com/datasets/rmisra/news-category-dataset",
            "https://www.kaggle.com/datasets/uom190346a/global-coffee-health-dataset",
            "https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers",
            "https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset",
            "https://www.kaggle.com/datasets/binovi/wholesale-customers-data-set",
            "https://www.kaggle.com/datasets/plantvillage-dataset",
            "https://www.kaggle.com/datasets/cpluzshrijayan/milkquality"
        ],
        "Aplicaciones_ML_No_Supervisado": [
            "Segmentaci√≥n clientes, Patrones compra",
            "Clustering sonidos, Clasificaci√≥n no supervisada",
            "Segmentaci√≥n personalidad, Perfiles cliente",
            "Topic modeling, Clustering temas",
            "Clustering geogr√°fico, Patrones consumo",
            "Detecci√≥n fraudes, Clustering crediticio",
            "Patrones sue√±o, Clustering estilos vida",
            "Segmentaci√≥n B2B, Patrones compra",
            "Clustering visual, Detecci√≥n anomal√≠as",
            "Control calidad, Detecci√≥n anomal√≠as"
        ]
    }

    df_summary = pd.DataFrame(datasets_info)

    # Estilo visual con pandas Styler
    styled_df = (
        df_summary.style
        .set_table_styles([
            {"selector": "thead th", "props": [("background-color", "#4C72B0"), ("color", "white"), ("font-weight", "bold")]},
            {"selector": "tbody tr:nth-child(even)", "props": [("background-color", "#f2f2f2")]},
            {"selector": "tbody tr:nth-child(odd)", "props": [("background-color", "white")]}
        ])
        .set_properties(**{
            "text-align": "left",
            "border": "1px solid #ddd",
            "color": "#222222"
        })
        .hide(axis="index")
    )

    show_section("Tabla resumen de los 10 datasets", emoji="üìã")
    display(styled_df)

    # Resumen general
    blank()
    print_kv("Total de datasets analizados", len(df_summary))
    print_kv("Dominios cubiertos", df_summary["Dominio"].nunique())
    print_kv("Tipos de formato", ", ".join(df_summary["Formato_Archivo"].unique()))

    return df_summary


# -------------------- Funci√≥n combinada de Dairy Dataset -------------------- #
def analyze_dairy_dataset(df: pd.DataFrame) -> None:
    """Muestra descripci√≥n detallada y realiza EDA visual del Dairy Goods Sales Dataset."""
    df.columns = df.columns.str.strip()  # limpiar espacios

    # ===== Carga y descripci√≥n ===== #
    show_subsection("Carga y descripci√≥n del conjunto de datos", emoji="‚úÖ")
    print_kv("Filas √ó Columnas", f"{df.shape[0]:,} √ó {df.shape[1]}")
    print_kv("Uso de memoria", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")

    show_subsection("Columnas disponibles", emoji="üìã")
    for i, col in enumerate(df.columns, 1):
        dtype = str(df[col].dtype)
        unique_vals = df[col].nunique() if df[col].dtype in ['object', 'int64'] else 'continua'
        print(f"  {i:2d}. {col:30s} ({dtype:8s}) - {unique_vals} valores √∫nicos")

    show_subsection("Estad√≠sticas clave", emoji="üìà")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print_kv("Variables num√©ricas", len(numeric_cols))
        for col in numeric_cols[:5]:  # primeras 5 para no saturar
            min_val, max_val = df[col].min(), df[col].max()
            mean_val = df[col].mean()
            print(f"  * {col}: min={min_val}, max={max_val}, promedio={mean_val:.2f}")

    null_counts = df.isnull().sum()
    completeness = (1 - null_counts.sum() / (df.shape[0] * df.shape[1])) * 100
    print_kv("Completitud", f"{completeness:.1f}%")

    # ===== EDA visual ===== #
    show_subsection("EDA visual", emoji="üìä")
    sns.set(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(16,10))

    # 1Ô∏è‚É£ Ventas totales por producto
    sns.barplot(
        x='Product Name', y='Quantity Sold (liters/kg)', 
        data=df, estimator=sum, ax=axes[0,0], palette='viridis'
    )
    axes[0,0].set_title("Ventas totales por Producto")
    axes[0,0].tick_params(axis='x', rotation=45)

    # 2Ô∏è‚É£ Distribuci√≥n de ventas por canal de venta
    sns.boxplot(
        x='Sales Channel', y='Quantity Sold (liters/kg)',
        data=df, ax=axes[0,1], palette='Set2'
    )
    axes[0,1].set_title("Distribuci√≥n de Ventas por Canal")
    axes[0,1].tick_params(axis='x', rotation=45)

    # 3Ô∏è‚É£ Matriz de correlaci√≥n num√©rica
    numeric_cols = ['Quantity (liters/kg)', 'Price per Unit', 'Total Value', 'Shelf Life (days)', 'Quantity in Stock (liters/kg)']
    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', ax=axes[1,0], fmt='.2f', linewidths=0.5)
    axes[1,0].set_title("Matriz de Correlaci√≥n")

    # 4Ô∏è‚É£ Tendencia de ventas mensuales (corregida)
    try:
        df_temp = df.copy()  # No modificar el DataFrame original
        df_temp['Date'] = pd.to_datetime(df_temp['Date'])
        df_temp['Month_Str'] = df_temp['Date'].dt.strftime('%Y-%m')  # Formato string para evitar problemas con Period
        monthly_sales = df_temp.groupby('Month_Str')['Quantity Sold (liters/kg)'].sum().reset_index()
        
        # Convertir de vuelta a datetime para el gr√°fico
        monthly_sales['Month_Date'] = pd.to_datetime(monthly_sales['Month_Str'] + '-01')
        
        axes[1,1].plot(monthly_sales['Month_Date'], monthly_sales['Quantity Sold (liters/kg)'], 
                      marker='o', color='b', linewidth=2)
        axes[1,1].set_title("Ventas Mensuales Totales")
        axes[1,1].tick_params(axis='x', rotation=45)
        axes[1,1].grid(True, alpha=0.3)
    except Exception as e:
        # Si hay error en las fechas, mostrar un gr√°fico alternativo
        axes[1,1].text(0.5, 0.5, f'Gr√°fico temporal no disponible\n(Error: {str(e)[:50]}...)', 
                      ha='center', va='center', transform=axes[1,1].transAxes)
        axes[1,1].set_title("Ventas Mensuales (No disponible)")

    plt.tight_layout()
    plt.show()



    # ======================================
# NUEVA FUNCI√ìN: ANALYZE ANIMAL SOUNDS DATASET
# ======================================

def analyze_animal_sounds_dataset(dataset_path: Path) -> None:
    if not dataset_path.exists():
        print("‚ö†Ô∏è Dataset no encontrado, disponible en:")
        print("https://github.com/YashNita/Animal-Sound-Dataset")
        return

    show_section("Animal Sounds Dataset", emoji="üêæ")

    # Traducci√≥n de nombres al castellano
    animal_names = {
        'Aslan': 'Le√≥n',
        'Esek': 'Burro', 
        'Inek': 'Vaca',
        'Kedi-Part1': 'Gato (Parte 1)',
        'Kedi-Part2': 'Gato (Parte 2)',
        'Kopek-Part1': 'Perro (Parte 1)',
        'Kopek-Part2': 'Perro (Parte 2)',
        'Koyun': 'Oveja',
        'Kurbaga': 'Rana',
        'Kus-Part1': 'P√°jaro (Parte 1)',
        'Kus-Part2': 'P√°jaro (Parte 2)',
        'Maymun': 'Mono',
        'Tavuk': 'Pollo'
    }

    # 1Ô∏è‚É£ Descripci√≥n general y estructura
    folders = [f for f in dataset_path.iterdir() if f.is_dir()]
    show_subsection("Descripci√≥n del dataset", emoji="üìä")
    print_kv("N√∫mero de categor√≠as de animales", len(folders))

    total_files = 0
    category_info = {}
    audio_samples = {}  # Para almacenar ejemplos de audio por animal
    
    for folder in folders:
        audio_files = list(folder.glob("*.wav"))
        total_files += len(audio_files)
        category_info[folder.name] = len(audio_files)
        # Guardar el primer archivo como ejemplo para reproducci√≥n
        if audio_files:
            audio_samples[folder.name] = audio_files[0]

    print_kv("Total de archivos de audio", f"{total_files:,}")

    show_subsection("Distribuci√≥n por especies", emoji="üì¶")
    for animal, count in sorted(category_info.items()):
        animal_castellano = animal_names.get(animal, animal)
        print(f"- {animal_castellano}: {count} archivos")

    # 2Ô∏è‚É£ Reproducci√≥n interactiva de sonidos de ejemplo
    show_subsection("üéµ Ejemplos de Sonidos por Especie", emoji="üîä")
    print("üìå Especies disponibles en el dataset:")
    print()
    
    # Mostrar opciones disponibles en castellano
    available_animals = list(audio_samples.keys())
    for i, animal in enumerate(available_animals, 1):
        animal_castellano = animal_names.get(animal, animal)
        print(f"   {i}. {animal_castellano} ({animal})")
    
    print(f"\nüí° Para reproducir un sonido, ejecuta en la siguiente celda:")
    print(f"   # Ejemplo: Reproducir sonido del animal en posici√≥n 1")
    print(f"   from IPython.display import Audio, display")
    print(f"   from pathlib import Path")
    print(f"   ")
    print(f"   # Opciones disponibles:")
    for i, animal in enumerate(available_animals, 1):
        if animal in audio_samples:
            file_path = audio_samples[animal]
            animal_castellano = animal_names.get(animal, animal)
            print(f"   # {i}. {animal_castellano}: display(Audio('{file_path}'))")
    
    print(f"\nüéß C√≥digo de ejemplo para reproducir cualquier sonido:")
    print(f"   animal_elegido = '{available_animals[0] if available_animals else 'Aslan'}'")
    print(f"   audio_path = Path('Animal Sound Dataset') / animal_elegido")
    print(f"   if audio_path.exists():")
    print(f"       audio_files = list(audio_path.glob('*.wav'))")
    print(f"       if audio_files:")
    print(f"           display(Audio(audio_files[0]))")

    # 3Ô∏è‚É£ Caracter√≠sticas t√©cnicas (muestra)
    if folders:
        sample_folder = folders[0]
        sample_files = list(sample_folder.glob("*.wav"))
        if sample_files:
            sample_file = sample_files[0]
            file_size = sample_file.stat().st_size
            show_subsection("Caracter√≠sticas t√©cnicas (muestra)", emoji="üîß")
            show_list([
                "Formato: WAV (Waveform Audio File Format)",
                f"Tama√±o aproximado del archivo de muestra: ~{file_size/1024:.1f} KB",
                "Tipo de datos: Se√±ales de audio digitales",
                "Dominio: Bioac√∫stica y reconocimiento de especies",
            ])
    else:
        print("‚ö†Ô∏è No se encontraron archivos de audio en las carpetas.")

    # 4Ô∏è‚É£ Posibles aplicaciones no supervisadas
    show_subsection("L√≠neas de an√°lisis no supervisado", emoji="üéØ")
    show_list([
        "Clustering espectral para identificar grupos ac√∫sticos",
        "Detecci√≥n de anomal√≠as en vocalizaciones",
        "An√°lisis de componentes principales en caracter√≠sticas MFCC",
        "Segmentaci√≥n autom√°tica de comportamientos vocales",
    ])

    # 5Ô∏è‚É£ EDA visual compacto con an√°lisis espectral
    show_subsection("EDA visual", emoji="üìä")
    fig, axes = plt.subplots(2, 2, figsize=(16,10))
    axes = axes.flatten()

    # Distribuci√≥n de cantidad de audios por categor√≠a (barplot)
    try:
        # Crear listas con nombres en castellano para el gr√°fico
        animals_castellano = [animal_names.get(animal, animal) for animal in category_info.keys()]
        counts = list(category_info.values())
        
        sns.barplot(
            x=animals_castellano, y=counts,
            ax=axes[0], palette='husl'
        )
        axes[0].set_title("Cantidad de archivos por especie")
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].set_ylabel("Cantidad de audios")
    except:
        axes[0].text(0.5,0.5,"No disponible", ha='center')

    # Ejemplo de espectrograma de un audio
    try:
        if 'sample_file' in locals():
            y, sr = librosa.load(sample_file, sr=None)
            S = librosa.feature.melspectrogram(y=y, sr=sr)
            S_dB = librosa.power_to_db(S, ref=np.max)
            librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', ax=axes[1])
            axes[1].set_title(f"Espectrograma: {sample_file.name}")
        else:
            axes[1].text(0.5,0.5,"No disponible", ha='center')
    except:
        axes[1].text(0.5,0.5,"No disponible", ha='center')

    # Forma de onda del audio
    try:
        if 'y' in locals() and 'sr' in locals():
            time_axis = np.linspace(0, len(y)/sr, len(y))
            axes[2].plot(time_axis, y, alpha=0.7)
            axes[2].set_title(f"Forma de onda: {sample_file.name}")
            axes[2].set_xlabel("Tiempo (s)")
            axes[2].set_ylabel("Amplitud")
        else:
            axes[2].text(0.5,0.5,"No disponible", ha='center')
    except:
        axes[2].text(0.5,0.5,"No disponible", ha='center')

    # An√°lisis de frecuencias (FFT)
    try:
        if 'y' in locals() and 'sr' in locals():
            fft = np.fft.fft(y)
            freq = np.fft.fftfreq(len(fft), 1/sr)
            magnitude = np.abs(fft)
            # Solo mostrar frecuencias positivas
            pos_mask = freq >= 0
            axes[3].plot(freq[pos_mask][:len(freq)//8], magnitude[pos_mask][:len(freq)//8])
            axes[3].set_title("Espectro de Frecuencias (FFT)")
            axes[3].set_xlabel("Frecuencia (Hz)")
            axes[3].set_ylabel("Magnitud")
        else:
            axes[3].text(0.5,0.5,"No disponible", ha='center')
    except:
        axes[3].text(0.5,0.5,"No disponible", ha='center')

    plt.tight_layout()
    plt.show()

    print()
    print("‚úÖ An√°lisis del Animal Sounds Dataset completado exitosamente")
    print("üéµ ¬°Usa el c√≥digo de ejemplo de arriba para reproducir sonidos de los animales!")


def play_animal_sounds_interactive(dataset_path: Path) -> None:
    """
    Funci√≥n interactiva para reproducir sonidos de animales del dataset.
    Reproduce autom√°ticamente un sonido aleatorio y muestra la forma de onda.
    """
    from IPython.display import Audio, display
    import random
    import librosa
    import matplotlib.pyplot as plt
    import numpy as np
    
    if not dataset_path.exists():
        print("‚ö†Ô∏è Dataset 'Animal Sound Dataset' no encontrado")
        print("üìÅ Verifica que la carpeta existe en el directorio actual")
        return

    show_section("Reproducci√≥n de Sonidos de Animales", emoji="üéµ")
    
    # Traducci√≥n de nombres al castellano
    animal_names = {
        'Aslan': 'Le√≥n',
        'Esek': 'Burro', 
        'Inek': 'Vaca',
        'Kedi-Part1': 'Gato (Parte 1)',
        'Kedi-Part2': 'Gato (Parte 2)',
        'Kopek-Part1': 'Perro (Parte 1)',
        'Kopek-Part2': 'Perro (Parte 2)',
        'Koyun': 'Oveja',
        'Kurbaga': 'Rana',
        'Kus-Part1': 'P√°jaro (Parte 1)',
        'Kus-Part2': 'P√°jaro (Parte 2)',
        'Maymun': 'Mono',
        'Tavuk': 'Pollo'
    }
    
    # Obtener todas las especies disponibles
    folders = [f for f in dataset_path.iterdir() if f.is_dir()]
    
    if not folders:
        print("‚ö†Ô∏è No se encontraron carpetas de animales")
        return
    
    # Reproducir un sonido aleatorio autom√°ticamente
    random_folder = random.choice(folders)
    audio_files = list(random_folder.glob("*.wav"))
    
    if audio_files:
        random_file = random.choice(audio_files)
        animal_castellano = animal_names.get(random_folder.name, random_folder.name)
        
        print("üéµ **REPRODUCIENDO SONIDO ALEATORIO**")
        print("=" * 50)
        print(f"üêæ **Animal**: {animal_castellano}")
        print(f"üìÅ **Archivo**: {random_file.name}")
        print("=" * 50)
        
        try:
            # Cargar y analizar el audio
            y, sr = librosa.load(random_file, sr=None)
            duration = len(y) / sr
            
            print(f"üìä **Caracter√≠sticas del audio**:")
            print(f"   ‚Ä¢ Duraci√≥n: {duration:.2f} segundos")
            print(f"   ‚Ä¢ Frecuencia de muestreo: {sr:,} Hz")
            print(f"   ‚Ä¢ N√∫mero de muestras: {len(y):,}")
            
            blank()
            
            # Crear visualizaci√≥n de la forma de onda
            fig, ax = plt.subplots(1, 1, figsize=(12, 4))
            time = np.linspace(0, duration, len(y))
            ax.plot(time, y, color='steelblue', alpha=0.8)
            ax.set_title(f'Forma de Onda - {animal_castellano}', fontsize=14, fontweight='bold')
            ax.set_xlabel('Tiempo (segundos)')
            ax.set_ylabel('Amplitud')
            ax.grid(True, alpha=0.3)
            ax.set_xlim(0, duration)
            plt.tight_layout()
            plt.show()
            
            blank()
            print("üîä **Reproduciendo audio...**")
            display(Audio(random_file))
            blank()
            print("‚úÖ **Audio reproducido exitosamente**")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error al procesar el audio: {e}")
            print("üîä **Reproduciendo audio...**")
            display(Audio(random_file))
            print("‚úÖ **Audio reproducido exitosamente**")
    else:
        print("‚ö†Ô∏è No se encontraron archivos de audio en las carpetas")


# ======================================
# NUEVA FUNCI√ìN: ANALYZE CUSTOMER PERSONALITY DATASET
# ======================================

def analyze_customer_personality_dataset(df: pd.DataFrame) -> None:
    """Muestra descripci√≥n detallada y realiza EDA visual del Customer Personality Analysis Dataset."""
    
    # ===== Carga y descripci√≥n ===== #
    show_subsection("Carga y descripci√≥n del conjunto de datos", emoji="‚úÖ")
    print_kv("Filas √ó Columnas", f"{df.shape[0]:,} √ó {df.shape[1]}")
    print_kv("Uso de memoria", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")

    show_subsection("Columnas disponibles", emoji="üìã")
    for i, col in enumerate(df.columns, 1):
        dtype = str(df[col].dtype)
        unique_vals = df[col].nunique() if df[col].dtype in ['object', 'int64'] else 'continua'
        print(f"  {i:2d}. {col:30s} ({dtype:8s}) - {unique_vals} valores √∫nicos")

    # ===== An√°lisis demogr√°fico ===== #
    show_subsection("An√°lisis demogr√°fico", emoji="üë•")
    
    # Crear variables derivadas para el an√°lisis
    df_analysis = df.copy()
    current_year = 2024
    df_analysis['Age'] = current_year - df_analysis['Year_Birth']
    
    # Variables de gasto total
    spending_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
    df_analysis['Total_Spending'] = df_analysis[spending_cols].sum(axis=1)
    
    # Variables de compras por canal
    purchase_cols = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']
    df_analysis['Total_Purchases'] = df_analysis[purchase_cols].sum(axis=1)
    
    print_kv("Edad promedio", f"{df_analysis['Age'].mean():.1f} a√±os")
    print_kv("Rango de edad", f"{df_analysis['Age'].min()} - {df_analysis['Age'].max()} a√±os")
    print_kv("Gasto promedio total", f"${df_analysis['Total_Spending'].mean():.2f}")
    print_kv("Ingreso promedio", f"${df_analysis['Income'].mean():.2f}")
    
    # Estad√≠sticas por categor√≠as
    show_subsection("Distribuci√≥n por categor√≠as", emoji="üìä")
    print_kv("Niveles educativos", df_analysis['Education'].value_counts().to_dict())
    print_kv("Estados civiles", df_analysis['Marital_Status'].value_counts().to_dict())
    
    # Valores nulos y completitud
    null_counts = df_analysis.isnull().sum()
    completeness = (1 - null_counts.sum() / (df_analysis.shape[0] * df_analysis.shape[1])) * 100
    print_kv("Completitud", f"{completeness:.1f}%")
    if null_counts.sum() > 0:
        print_kv("Columnas con valores nulos", null_counts[null_counts > 0].to_dict())

    # ===== EDA visual ===== #
    show_subsection("EDA visual", emoji="üìà")
    sns.set(style="whitegrid")
    fig, axes = plt.subplots(2, 2, figsize=(16,10))

    # 1Ô∏è‚É£ Distribuci√≥n de edad vs ingreso
    scatter = axes[0,0].scatter(df_analysis['Age'], df_analysis['Income'], 
                               c=df_analysis['Total_Spending'], cmap='viridis', alpha=0.6)
    axes[0,0].set_xlabel('Edad')
    axes[0,0].set_ylabel('Ingreso')
    axes[0,0].set_title('Edad vs Ingreso (coloreado por Gasto Total)')
    plt.colorbar(scatter, ax=axes[0,0])

    # 2Ô∏è‚É£ Gasto por categor√≠a de producto
    spending_means = df_analysis[spending_cols].mean()
    sns.barplot(x=spending_means.index, y=spending_means.values, ax=axes[0,1], palette='Set2')
    axes[0,1].set_title('Gasto Promedio por Categor√≠a de Producto')
    axes[0,1].tick_params(axis='x', rotation=45)
    axes[0,1].set_ylabel('Gasto Promedio ($)')

    # 3Ô∏è‚É£ Matriz de correlaci√≥n de variables de gasto
    corr_data = df_analysis[spending_cols + ['Income', 'Total_Spending']].corr()
    sns.heatmap(corr_data, annot=True, cmap='coolwarm', ax=axes[1,0], 
                fmt='.2f', linewidths=0.5, square=True)
    axes[1,0].set_title('Correlaci√≥n entre Variables de Gasto e Ingreso')

    # 4Ô∏è‚É£ Distribuci√≥n de canales de compra
    purchase_means = df_analysis[purchase_cols].mean()
    sns.barplot(x=purchase_means.index, y=purchase_means.values, ax=axes[1,1], palette='husl')
    axes[1,1].set_title('Compras Promedio por Canal')
    axes[1,1].tick_params(axis='x', rotation=45)
    axes[1,1].set_ylabel('Compras Promedio')

    plt.tight_layout()
    plt.show()

    # ===== An√°lisis de segmentaci√≥n potencial ===== #
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Segmentaci√≥n por patrones de gasto y preferencias de producto",
        "Clustering por canales de compra preferidos (online vs offline)",
        "Identificaci√≥n de perfiles demogr√°ficos por comportamiento",
        "Detecci√≥n de clientes de alto valor (outliers en gasto)",
        "An√°lisis de respuesta a campa√±as de marketing",
    ])

    show_subsection("T√©cnicas recomendadas", emoji="üî¨")
    show_list([
        "K-Means para segmentaci√≥n cl√°sica de clientes",
        "DBSCAN para identificar clientes at√≠picos de alto/bajo valor",
        "PCA para reducir dimensionalidad de variables de gasto",
        "Gaussian Mixture Models para perfiles probabil√≠sticos",
    ])

    print()
    print("‚úÖ An√°lisis del Customer Personality Analysis Dataset completado exitosamente")


# ======================================
# FUNCIONES DE AN√ÅLISIS TE√ìRICO (DATASETS SIN DATOS LOCALES)
# ======================================

def analyze_news_category_dataset() -> None:
    """An√°lisis te√≥rico del News Category Dataset basado en fuente web."""
    
    show_subsection("An√°lisis basado en fuente web", emoji="üåê")
    print_kv("URL de origen", "https://www.kaggle.com/datasets/rmisra/news-category-dataset")
    print_kv("Estado del dataset", "No disponible localmente - An√°lisis te√≥rico")
    
    show_subsection("Caracter√≠sticas esperadas del dataset", emoji="üìä")
    print_kv("Tama√±o estimado", "~210,000 art√≠culos de noticias (HuffPost)")
    print_kv("Formato", "JSON")
    print_kv("Per√≠odo temporal", "2012-2022")
    print_kv("Variables principales", "headline, short_description, category, authors, date")
    
    show_subsection("Estructura de datos esperada", emoji="üìã")
    expected_columns = [
        "headline (t√≠tulo del art√≠culo)",
        "short_description (descripci√≥n breve)",
        "category (categor√≠a de noticia)",
        "authors (autores)",
        "date (fecha de publicaci√≥n)",
        "link (enlace web)"
    ]
    for i, col in enumerate(expected_columns, 1):
        print(f"  {i}. {col}")
    
    show_subsection("An√°lisis de dominio", emoji="üì∞")
    print_kv("Diversidad tem√°tica", "Pol√≠tica, entretenimiento, deportes, tecnolog√≠a, mundo, etc.")
    print_kv("Longitud promedio titular", "~50 caracteres")
    print_kv("Longitud promedio descripci√≥n", "~200 caracteres")
    print_kv("Idioma", "Ingl√©s")
    
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Topic Modeling con LDA para descubrir temas emergentes",
        "Clustering sem√°ntico basado en embeddings (Word2Vec/BERT)",
        "Detecci√≥n autom√°tica de tendencias noticiosas",
        "An√°lisis de sentimientos no supervisado",
        "Segmentaci√≥n de audiencias por preferencias tem√°ticas"
    ])
    
    show_subsection("T√©cnicas aplicables", emoji="üî¨")
    show_list([
        "TF-IDF + K-Means para clustering sem√°ntico",
        "NMF (Non-negative Matrix Factorization) para topic modeling",
        "DBSCAN para detecci√≥n de noticias at√≠picas",
        "t-SNE/UMAP para visualizaci√≥n de clusters textuales"
    ])
    
    print()
    print("‚úÖ An√°lisis te√≥rico del News Category Dataset completado")


def analyze_coffee_health_dataset() -> None:
    """An√°lisis te√≥rico del Global Coffee Health Dataset basado en fuente web."""
    
    show_subsection("An√°lisis basado en fuente web", emoji="üåê")
    print_kv("URL de origen", "https://www.kaggle.com/datasets/uom190346a/global-coffee-health-dataset")
    print_kv("Estado del dataset", "No disponible localmente - An√°lisis te√≥rico")
    
    show_subsection("Caracter√≠sticas esperadas del dataset", emoji="üìä")
    print_kv("Alcance geogr√°fico", "~180+ pa√≠ses/regiones globales")
    print_kv("Per√≠odo temporal", "Datos longitudinales 2000-2020")
    print_kv("Tipo de datos", "Num√©rico/geogr√°fico con indicadores de salud")
    print_kv("Variables principales", "consumo per c√°pita, esperanza de vida, indicadores socioecon√≥micos")
    
    show_subsection("Variables esperadas por categor√≠a", emoji="‚òï")
    health_vars = [
        "Coffee_Consumption_Per_Capita (kg/a√±o)",
        "Life_Expectancy (a√±os)",
        "Heart_Disease_Rate (%)",
        "Diabetes_Prevalence (%)",
        "GDP_Per_Capita (USD)",
        "Healthcare_Index (0-100)"
    ]
    for i, var in enumerate(health_vars, 1):
        print(f"  {i}. {var}")
    
    show_subsection("Cobertura regional esperada", emoji="üó∫Ô∏è")
    print_kv("Regiones incluidas", "Europa, Am√©rica, Asia, √Åfrica, Ocean√≠a")
    print_kv("Variabilidad cultural", "Diferentes patrones de consumo por regi√≥n")
    print_kv("Datos urbanos vs rurales", "Segmentaci√≥n por tipo de poblaci√≥n")
    
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Clustering geogr√°fico por patrones salud-consumo",
        "Detecci√≥n de pa√≠ses outliers con patrones √∫nicos",
        "PCA para identificar factores principales de salud global",
        "Segmentaci√≥n epidemiol√≥gica por perfiles regionales",
        "An√°lisis de correlaciones no lineales caf√©-salud"
    ])
    
    show_subsection("T√©cnicas recomendadas", emoji="üî¨")
    show_list([
        "K-Means para agrupaci√≥n geogr√°fica de pa√≠ses",
        "DBSCAN para detectar pa√≠ses con patrones at√≠picos",
        "Hierarchical clustering para taxonom√≠a regional",
        "Gaussian Mixture Models para perfiles probabil√≠sticos"
    ])
    
    print()
    print("‚úÖ An√°lisis te√≥rico del Global Coffee Health Dataset completado")


def analyze_credit_card_dataset() -> None:
    """An√°lisis te√≥rico del Credit Card Customer Dataset basado en fuente web."""
    
    show_subsection("An√°lisis basado en fuente web", emoji="üåê")
    print_kv("URL de origen", "https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers")
    print_kv("Estado del dataset", "No disponible localmente - An√°lisis te√≥rico")
    
    show_subsection("Caracter√≠sticas esperadas del dataset", emoji="üìä")
    print_kv("Tama√±o estimado", "~10,000 registros de clientes")
    print_kv("Variables esperadas", "20+ columnas financieras y demogr√°ficas")
    print_kv("Dominio", "Servicios financieros - Gesti√≥n de riesgo crediticio")
    
    show_subsection("Variables financieras clave esperadas", emoji="üí≥")
    financial_vars = [
        "Credit_Limit (l√≠mite de cr√©dito autorizado)",
        "Total_Revolving_Bal (balance rotativo actual)",
        "Total_Trans_Amt (monto total de transacciones)",
        "Total_Trans_Ct (cantidad de transacciones)",
        "Avg_Utilization_Ratio (ratio de utilizaci√≥n promedio)",
        "Customer_Age (edad del cliente)",
        "Income_Category (categor√≠a de ingresos)",
        "Card_Category (tipo de tarjeta)"
    ]
    for i, var in enumerate(financial_vars, 1):
        print(f"  {i}. {var}")
    
    show_subsection("Perfiles de clientes esperados", emoji="üë•")
    print_kv("Segmentos demogr√°ficos", "Edad, g√©nero, estado civil, educaci√≥n")
    print_kv("Comportamiento crediticio", "Usuarios activos vs inactivos")
    print_kv("Patrones de gasto", "Categor√≠as de transacciones y frecuencia")
    
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Detecci√≥n de anomal√≠as para prevenci√≥n de fraudes",
        "Segmentaci√≥n autom√°tica por riesgo crediticio",
        "Clustering de comportamientos de gasto similares",
        "Identificaci√≥n de patrones de abandono de clientes",
        "Perfilado no supervisado para productos financieros"
    ])
    
    show_subsection("T√©cnicas aplicables", emoji="üî¨")
    show_list([
        "Isolation Forest para detecci√≥n de transacciones fraudulentas",
        "K-Means para segmentaci√≥n de riesgo crediticio",
        "DBSCAN para identificar comportamientos at√≠picos",
        "PCA para an√°lisis de factores de riesgo principales"
    ])
    
    print()
    print("‚úÖ An√°lisis te√≥rico del Credit Card Customer Dataset completado")


def analyze_sleep_health_dataset() -> None:
    """An√°lisis te√≥rico del Sleep Health and Lifestyle Dataset basado en fuente web."""
    
    show_subsection("An√°lisis basado en fuente web", emoji="üåê")
    print_kv("URL de origen", "https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset")
    print_kv("Estado del dataset", "No disponible localmente - An√°lisis te√≥rico")
    
    show_subsection("Caracter√≠sticas esperadas del dataset", emoji="üìä")
    print_kv("Tama√±o estimado", "~400 registros de individuos")
    print_kv("Variables esperadas", "13 columnas (sue√±o, f√≠sicas, estilo de vida)")
    print_kv("Dominio", "Medicina del sue√±o y bienestar personal")
    
    show_subsection("Variables de sue√±o y salud esperadas", emoji="üò¥")
    sleep_vars = [
        "Sleep_Duration (horas de sue√±o por noche)",
        "Quality_of_Sleep (calidad percibida 1-10)",
        "Physical_Activity_Level (minutos ejercicio/d√≠a)",
        "Stress_Level (nivel de estr√©s 1-10)",
        "BMI_Category (categor√≠a √≠ndice masa corporal)",
        "Heart_Rate (frecuencia card√≠aca en bpm)",
        "Daily_Steps (pasos diarios promedio)",
        "Sleep_Disorder (trastorno del sue√±o diagnosticado)"
    ]
    for i, var in enumerate(sleep_vars, 1):
        print(f"  {i}. {var}")
    
    show_subsection("Perfiles de sue√±o esperados", emoji="üõå")
    print_kv("Cronotipos", "Ma√±aneros, nocturnos, intermedios")
    print_kv("Calidad del sue√±o", "Buena, regular, mala")
    print_kv("Trastornos", "Insomnio, apnea del sue√±o, sin trastornos")
    
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Clustering de cronotipos naturales sin etiquetas previas",
        "Detecci√≥n de patrones de insomnio no diagnosticados",
        "Identificaci√≥n de factores de riesgo combinados",
        "Segmentaci√≥n por perfiles de bienestar integral",
        "An√°lisis de correlaciones ocultas sue√±o-salud"
    ])
    
    show_subsection("T√©cnicas recomendadas", emoji="üî¨")
    show_list([
        "K-Means para tipolog√≠a de dormidores",
        "Gaussian Mixture Models para cronotipos probabil√≠sticos",
        "DBSCAN para detecci√≥n de trastornos at√≠picos",
        "PCA para factores principales del bienestar"
    ])
    
    print()
    print("‚úÖ An√°lisis te√≥rico del Sleep Health and Lifestyle Dataset completado")


def analyze_wholesale_customers_dataset() -> None:
    """An√°lisis te√≥rico del Wholesale Customers Dataset basado en fuente web."""
    
    show_subsection("An√°lisis basado en fuente web", emoji="üåê")
    print_kv("URL de origen", "https://www.kaggle.com/datasets/binovi/wholesale-customers-data-set")
    print_kv("Estado del dataset", "No disponible localmente - An√°lisis te√≥rico")
    
    show_subsection("Caracter√≠sticas esperadas del dataset", emoji="üìä")
    print_kv("Tama√±o", "440 clientes mayoristas")
    print_kv("Origen", "UCI Machine Learning Repository (dataset cl√°sico)")
    print_kv("Sector", "Distribuci√≥n B2B de productos de consumo masivo")
    print_kv("Variables", "8 columnas (Channel, Region + 6 categor√≠as de productos)")
    
    show_subsection("Categor√≠as de productos esperadas", emoji="üì¶")
    product_categories = [
        "Fresh (productos frescos: frutas, verduras, carnes)",
        "Milk (productos l√°cteos y derivados)",
        "Grocery (comestibles secos y enlatados)",
        "Frozen (productos congelados)",
        "Detergents_Paper (productos de limpieza y papel)",
        "Delicatessen (productos gourmet y especialidades)"
    ]
    for i, cat in enumerate(product_categories, 1):
        print(f"  {i}. {cat}")
    
    show_subsection("Segmentaci√≥n comercial esperada", emoji="üè¢")
    print_kv("Canales de venta", "Horeca (hoteles/restaurantes) vs Retail (minoristas)")
    print_kv("Regiones", "Diferentes √°reas geogr√°ficas de distribuci√≥n")
    print_kv("Patrones de compra", "Estacionales vs constantes por categor√≠a")
    
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Segmentaci√≥n cl√°sica de clientes B2B por volumen de compra",
        "Identificaci√≥n de patrones estacionales sin supervisi√≥n",
        "Clustering para estrategias de cross-selling autom√°tico",
        "Detecci√≥n de clientes at√≠picos (outliers de compra)",
        "An√°lisis de canasta de mercado mayorista"
    ])
    
    show_subsection("T√©cnicas aplicables", emoji="üî¨")
    show_list([
        "K-Means para segmentaci√≥n cl√°sica (caso de estudio t√≠pico)",
        "Hierarchical clustering para taxonom√≠a de compradores",
        "PCA para reducci√≥n dimensional de categor√≠as de productos",
        "DBSCAN para identificar nichos de mercado espec√≠ficos"
    ])
    
    print()
    print("‚úÖ An√°lisis te√≥rico del Wholesale Customers Dataset completado")


# ======================================
# NUEVA FUNCI√ìN: ANALYZE PLANTVILLAGE DATASET
# ======================================

def analyze_plantvillage_dataset(dataset_path: str) -> None:
    """An√°lisis completo del PlantVillage Dataset con EDA visual avanzado."""
    import os
    import random
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from PIL import Image, ImageStat
    from matplotlib.gridspec import GridSpec
    from collections import Counter
    
    if not os.path.exists(dataset_path):
        print("‚ö†Ô∏è Dataset no encontrado en la ruta especificada")
        return

    # ===== Carga y descripci√≥n ===== #
    show_subsection("Carga y descripci√≥n del conjunto de datos", emoji="‚úÖ")
    
    # Listar carpetas (clases)
    classes = sorted([d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))])
    num_classes = len(classes)
    
    print_kv("Ubicaci√≥n", dataset_path)
    print_kv("Clases detectadas", num_classes)
    
    # Contar im√°genes por clase y calcular estad√≠sticas
    class_counts = {}
    total_size_bytes = 0
    
    for cls in classes:
        folder = os.path.join(dataset_path, cls)
        images = [f for f in os.listdir(folder) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
        class_counts[cls] = len(images)
        
        # Calcular tama√±o de carpeta
        for img_file in images[:5]:  # solo algunos archivos para optimizar
            try:
                total_size_bytes += os.path.getsize(os.path.join(folder, img_file))
            except:
                continue

    # Convertir a DataFrame para an√°lisis
    df_counts = pd.DataFrame(list(class_counts.items()), columns=["Clase", "Cantidad"])
    total_images = df_counts["Cantidad"].sum()
    
    print_kv("Total de im√°genes", f"{total_images:,}")
    print_kv("Promedio por clase", f"{df_counts['Cantidad'].mean():.1f}")
    print_kv("Desviaci√≥n est√°ndar", f"{df_counts['Cantidad'].std():.1f}")
    print_kv("Tama√±o estimado", f"{(total_size_bytes * total_images / (5 * len(classes))) / (1024**2):.1f} MB")

    # ===== An√°lisis de distribuci√≥n ===== #
    show_subsection("An√°lisis de distribuci√≥n por clases", emoji="üìä")
    
    # Separar por tipo de planta y condici√≥n
    plant_types = {}
    conditions = Counter()
    
    for cls in classes:
        if '___' in cls:
            plant, condition = cls.split('___', 1)
            if plant not in plant_types:
                plant_types[plant] = []
            plant_types[plant].append(condition)
            conditions[condition] += class_counts[cls]
        else:
            # Para clases sin separador claro
            parts = cls.split('_')
            if len(parts) > 1:
                condition = parts[-1] if 'healthy' in parts[-1].lower() or 'disease' in parts[-1].lower() else 'other'
                conditions[condition] += class_counts[cls]
    
    print_kv("Tipos de plantas", len(plant_types))
    print_kv("Condiciones detectadas", len(conditions))
    print_kv("Distribuci√≥n de condiciones", dict(conditions.most_common(5)))

    # ===== EDA Visual Avanzado ===== #
    show_subsection("EDA Visual", emoji="üé®")
    
    # Crear figura con grid personalizado
    fig = plt.figure(figsize=(20, 16))
    gs = GridSpec(4, 4, figure=fig, hspace=0.3, wspace=0.3)
    
    # 1Ô∏è‚É£ Distribuci√≥n por clases (top 10)
    ax1 = fig.add_subplot(gs[0, :2])
    top_classes = df_counts.nlargest(10, 'Cantidad')
    bars = ax1.barh(top_classes['Clase'], top_classes['Cantidad'], 
                    color=plt.cm.viridis(np.linspace(0, 1, len(top_classes))))
    ax1.set_title('Top 10 Clases por Cantidad de Im√°genes', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Cantidad de Im√°genes')
    
    # Agregar valores en las barras
    for i, (bar, value) in enumerate(zip(bars, top_classes['Cantidad'])):
        ax1.text(bar.get_width() + 10, bar.get_y() + bar.get_height()/2, 
                str(value), va='center', ha='left', fontweight='bold')
    
    # 2Ô∏è‚É£ Histograma de distribuci√≥n
    ax2 = fig.add_subplot(gs[0, 2:])
    ax2.hist(df_counts['Cantidad'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')
    ax2.axvline(df_counts['Cantidad'].mean(), color='red', linestyle='--', 
               label=f'Media: {df_counts["Cantidad"].mean():.0f}')
    ax2.axvline(df_counts['Cantidad'].median(), color='orange', linestyle='--',
               label=f'Mediana: {df_counts["Cantidad"].median():.0f}')
    ax2.set_title('Distribuci√≥n de Im√°genes por Clase', fontweight='bold')
    ax2.set_xlabel('Cantidad de Im√°genes')
    ax2.set_ylabel('Frecuencia')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3Ô∏è‚É£ Mosaico de im√°genes representativas
    ax3 = fig.add_subplot(gs[1:3, :])
    ax3.axis('off')
    ax3.set_title('Mosaico de Muestras Representativas por Clase', fontsize=14, fontweight='bold', pad=20)
    
    # Seleccionar 12 clases aleatorias para el mosaico
    sample_classes = random.sample(classes, min(12, len(classes)))
    
    for i, cls in enumerate(sample_classes):
        try:
            folder = os.path.join(dataset_path, cls)
            images = [f for f in os.listdir(folder) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
            if images:
                sample_image = random.choice(images)
                img_path = os.path.join(folder, sample_image)
                img = Image.open(img_path)
                
                # Crear subplot para cada imagen
                row = i // 4 + 1
                col = i % 4
                ax_img = plt.subplot(4, 4, row * 4 + col + 1)
                ax_img.imshow(img)
                ax_img.axis('off')
                
                # T√≠tulo con informaci√≥n de la clase
                title = cls.replace('___', '\n').replace('_', ' ')
                if len(title) > 25:
                    title = title[:22] + "..."
                ax_img.set_title(title, fontsize=8, fontweight='bold')
        except Exception as e:
            continue
    
    # 4Ô∏è‚É£ An√°lisis de caracter√≠sticas de imagen
    ax4 = fig.add_subplot(gs[3, :2])
    
    # Calcular estad√≠sticas de color y tama√±o
    brightness_data = []
    size_data = []
    aspect_ratios = []
    
    sample_for_analysis = random.sample(classes, min(5, len(classes)))
    
    for cls in sample_for_analysis:
        folder = os.path.join(dataset_path, cls)
        images = [f for f in os.listdir(folder) if f.lower().endswith((".jpg", ".jpeg", ".png"))]
        
        for img_file in random.sample(images, min(3, len(images))):
            try:
                img_path = os.path.join(folder, img_file)
                with Image.open(img_path) as img:
                    # Brillo promedio
                    stat = ImageStat.Stat(img)
                    brightness = sum(stat.mean) / len(stat.mean)
                    brightness_data.append(brightness)
                    
                    # Dimensiones
                    width, height = img.size
                    size_data.append(width * height)
                    aspect_ratios.append(width / height)
            except:
                continue
    
    # Gr√°fico de dispersi√≥n: tama√±o vs brillo
    scatter = ax4.scatter(size_data, brightness_data, c=aspect_ratios, 
                         cmap='viridis', alpha=0.7, s=60)
    ax4.set_xlabel('Tama√±o (p√≠xeles)')
    ax4.set_ylabel('Brillo Promedio')
    ax4.set_title('Caracter√≠sticas de Imagen: Tama√±o vs Brillo', fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # Colorbar para aspect ratio
    cbar = plt.colorbar(scatter, ax=ax4)
    cbar.set_label('Aspect Ratio')
    
    # 5Ô∏è‚É£ Distribuci√≥n de condiciones (si aplicable)
    ax5 = fig.add_subplot(gs[3, 2:])
    
    if len(conditions) > 1:
        condition_names = list(conditions.keys())
        condition_counts = list(conditions.values())
        
        wedges, texts, autotexts = ax5.pie(condition_counts, labels=condition_names, 
                                          autopct='%1.1f%%', startangle=90,
                                          colors=plt.cm.Set3(np.linspace(0, 1, len(condition_names))))
        ax5.set_title('Distribuci√≥n por Condici√≥n de Salud', fontweight='bold')
        
        # Mejorar legibilidad del texto
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
    else:
        ax5.text(0.5, 0.5, 'An√°lisis de condiciones\nno disponible', 
                ha='center', va='center', transform=ax5.transAxes,
                fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        ax5.axis('off')
    
    plt.tight_layout()
    plt.show()
    
    # ===== Estad√≠sticas avanzadas ===== #
    show_subsection("Estad√≠sticas t√©cnicas", emoji="üìê")
    
    if size_data and brightness_data:
        print_kv("Resoluci√≥n promedio", f"{np.mean([np.sqrt(s) for s in size_data]):.0f} px (equiv.)")
        print_kv("Brillo promedio", f"{np.mean(brightness_data):.1f}")
        print_kv("Aspect ratio promedio", f"{np.mean(aspect_ratios):.2f}")
        print_kv("Variabilidad de tama√±o", f"CV = {np.std(size_data)/np.mean(size_data)*100:.1f}%")
    
    # ===== Potencial para ML No Supervisado ===== #
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "Clustering visual por caracter√≠sticas de color y textura",
        "Detecci√≥n de anomal√≠as en hojas con patrones at√≠picos",
        "Reducci√≥n dimensional con autoencoders para embeddings de im√°genes",
        "Segmentaci√≥n no supervisada por severidad de enfermedad",
        "An√°lisis de componentes principales en caracter√≠sticas visuales"
    ])
    
    show_subsection("T√©cnicas recomendadas", emoji="üî¨")
    show_list([
        "K-Means sobre caracter√≠sticas CNN pre-entrenadas (ResNet, VGG)",
        "DBSCAN para detecci√≥n de im√°genes outliers o mal etiquetadas",
        "t-SNE/UMAP para visualizaci√≥n de clusters en espacio de caracter√≠sticas",
        "Autoencoders variacionales para generaci√≥n y clustering latente"
    ])
    
    print()
    print("‚úÖ An√°lisis del PlantVillage Dataset completado exitosamente")


# ======================================
# NUEVA FUNCI√ìN: ANALYZE MILK QUALITY DATASET
# ======================================

def analyze_milk_quality_dataset(file_path: str) -> None:
    """An√°lisis completo del Milk Quality Dataset con EDA visual avanzado."""
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.gridspec import GridSpec
    from scipy import stats
    
    try:
        # ===== Carga y descripci√≥n ===== #
        df = pd.read_csv(file_path)
        
        show_subsection("Carga y descripci√≥n del conjunto de datos", emoji="‚úÖ")
        print_kv("Ubicaci√≥n", file_path)
        print_kv("Tama√±o", f"{df.shape[0]:,} filas √ó {df.shape[1]} columnas")
        print_kv("Uso de memoria", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")

        show_subsection("Columnas disponibles", emoji="üìã")
        for i, col in enumerate(df.columns, 1):
            dtype = str(df[col].dtype)
            unique_vals = df[col].nunique() if df[col].dtype in ['object', 'int64'] else 'continua'
            print(f"  {i:2d}. {col:25s} ({dtype:8s}) - {unique_vals} valores √∫nicos")

        # ===== An√°lisis estad√≠stico ===== #
        show_subsection("An√°lisis estad√≠stico", emoji="üìà")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        print_kv("Variables num√©ricas", len(numeric_cols))
        print_kv("Variables categ√≥ricas", len(categorical_cols))
        
        # Estad√≠sticas b√°sicas para variables num√©ricas
        if numeric_cols:
            for col in numeric_cols[:6]:  # Mostrar primeras 6
                mean_val = df[col].mean()
                std_val = df[col].std()
                min_val, max_val = df[col].min(), df[col].max()
                print(f"  * {col}: Œº={mean_val:.2f} (¬±{std_val:.2f}), rango=[{min_val:.2f}, {max_val:.2f}]")

        # Verificar valores nulos y completitud
        null_counts = df.isnull().sum()
        completeness = (1 - null_counts.sum() / (df.shape[0] * df.shape[1])) * 100
        print_kv("Completitud", f"{completeness:.1f}%")
        
        if null_counts.sum() > 0:
            print_kv("Columnas con nulos", {col: count for col, count in null_counts.items() if count > 0})

        # ===== EDA Visual Avanzado ===== #
        show_subsection("EDA Visual", emoji="üìä")
        
        # Crear figura con grid complejo
        fig = plt.figure(figsize=(20, 16))
        gs = GridSpec(4, 4, figure=fig, hspace=0.4, wspace=0.3)
        
        # 1Ô∏è‚É£ Distribuciones de variables principales
        ax1 = fig.add_subplot(gs[0, :2])
        
        # Identificar variables principales (primeras num√©ricas)
        main_vars = numeric_cols[:4] if len(numeric_cols) >= 4 else numeric_cols
        
        for i, var in enumerate(main_vars):
            ax1.hist(df[var], alpha=0.6, label=var, bins=20)
        
        ax1.set_title('Distribuciones de Variables Fisicoqu√≠micas Principales', fontweight='bold')
        ax1.set_xlabel('Valores')
        ax1.set_ylabel('Frecuencia')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2Ô∏è‚É£ Matriz de correlaci√≥n
        ax2 = fig.add_subplot(gs[0, 2:])
        
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            
            sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                       center=0, ax=ax2, fmt='.2f', linewidths=0.5)
            ax2.set_title('Matriz de Correlaci√≥n entre Variables', fontweight='bold')
        else:
            ax2.text(0.5, 0.5, 'Matriz de correlaci√≥n\nno disponible', 
                    ha='center', va='center', transform=ax2.transAxes)
        
        # 3Ô∏è‚É£ Boxplots para detectar outliers
        ax3 = fig.add_subplot(gs[1, :])
        
        if main_vars:
            # Normalizar datos para mejor visualizaci√≥n
            df_norm = df[main_vars].copy()
            for col in main_vars:
                df_norm[col] = (df[col] - df[col].mean()) / df[col].std()
            
            bp = ax3.boxplot([df_norm[col].dropna() for col in main_vars], 
                            labels=main_vars, patch_artist=True)
            
            # Colorear boxplots
            colors = plt.cm.Set3(np.linspace(0, 1, len(main_vars)))
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            
            ax3.set_title('Detecci√≥n de Outliers (Valores Normalizados)', fontweight='bold')
            ax3.set_ylabel('Valores Z-Score')
            ax3.grid(True, alpha=0.3)
            ax3.tick_params(axis='x', rotation=45)
        
        # 4Ô∏è‚É£ An√°lisis de calidad (si existe variable de calidad)
        ax4 = fig.add_subplot(gs[2, :2])
        
        # Buscar variable de calidad/grade
        quality_col = None
        for col in df.columns:
            if any(keyword in col.lower() for keyword in ['grade', 'quality', 'class', 'type']):
                quality_col = col
                break
        
        if quality_col and quality_col in categorical_cols:
            quality_counts = df[quality_col].value_counts()
            wedges, texts, autotexts = ax4.pie(quality_counts.values, 
                                              labels=quality_counts.index,
                                              autopct='%1.1f%%', startangle=90,
                                              colors=plt.cm.Pastel1(np.linspace(0, 1, len(quality_counts))))
            ax4.set_title(f'Distribuci√≥n de {quality_col}', fontweight='bold')
            
            for autotext in autotexts:
                autotext.set_color('black')
                autotext.set_fontweight('bold')
        else:
            ax4.text(0.5, 0.5, 'An√°lisis de calidad\nno disponible\n(variable no identificada)', 
                    ha='center', va='center', transform=ax4.transAxes,
                    fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
            ax4.axis('off')
        
        # 5Ô∏è‚É£ Scatter plot multivariado
        ax5 = fig.add_subplot(gs[2, 2:])
        
        if len(main_vars) >= 2:
            x_var, y_var = main_vars[0], main_vars[1]
            
            # Color por tercera variable si est√° disponible
            if len(main_vars) >= 3:
                scatter = ax5.scatter(df[x_var], df[y_var], c=df[main_vars[2]], 
                                    cmap='viridis', alpha=0.6, s=50)
                plt.colorbar(scatter, ax=ax5, label=main_vars[2])
            else:
                ax5.scatter(df[x_var], df[y_var], alpha=0.6, s=50, color='skyblue')
            
            ax5.set_xlabel(x_var)
            ax5.set_ylabel(y_var)
            ax5.set_title(f'Relaci√≥n {x_var} vs {y_var}', fontweight='bold')
            ax5.grid(True, alpha=0.3)
            
            # Agregar l√≠nea de tendencia
            if df[x_var].notna().sum() > 1 and df[y_var].notna().sum() > 1:
                z = np.polyfit(df[x_var].dropna(), df[y_var].dropna(), 1)
                p = np.poly1d(z)
                ax5.plot(df[x_var].dropna().sort_values(), p(df[x_var].dropna().sort_values()), 
                        "r--", alpha=0.8, label=f'Tendencia (R¬≤‚âà{np.corrcoef(df[x_var].dropna(), df[y_var].dropna())[0,1]**2:.3f})')
                ax5.legend()
        
        # 6Ô∏è‚É£ An√°lisis de normalidad
        ax6 = fig.add_subplot(gs[3, :])
        
        if main_vars:
            # Test de normalidad para variables principales
            normality_results = []
            
            for var in main_vars[:4]:  # m√°ximo 4 variables
                data = df[var].dropna()
                if len(data) > 3:
                    statistic, p_value = stats.shapiro(data[:5000])  # l√≠mite para performance
                    is_normal = p_value > 0.05
                    normality_results.append({
                        'Variable': var,
                        'Statistic': statistic,
                        'P-value': p_value,
                        'Normal': 'S√≠' if is_normal else 'No'
                    })
            
            # Crear tabla de normalidad
            if normality_results:
                norm_df = pd.DataFrame(normality_results)
                ax6.axis('tight')
                ax6.axis('off')
                table = ax6.table(cellText=norm_df.values,
                                colLabels=norm_df.columns,
                                cellLoc='center',
                                loc='center',
                                bbox=[0, 0, 1, 1])
                table.auto_set_font_size(False)
                table.set_fontsize(10)
                table.scale(1, 2)
                
                # Colorear seg√∫n normalidad
                for i in range(len(norm_df)):
                    color = 'lightgreen' if norm_df.iloc[i]['Normal'] == 'S√≠' else 'lightcoral'
                    table[(i+1, 3)].set_facecolor(color)  # Columna 'Normal'
                
                ax6.set_title('Test de Normalidad (Shapiro-Wilk)', fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.show()
        
        # ===== Estad√≠sticas avanzadas ===== #
        show_subsection("Estad√≠sticas t√©cnicas", emoji="üìê")
        
        if numeric_cols:
            print_kv("Coeficiente de variaci√≥n promedio", f"{np.mean([df[col].std()/df[col].mean() for col in numeric_cols if df[col].mean() != 0]):.3f}")
            
            # Detectar outliers usando IQR
            total_outliers = 0
            for col in numeric_cols:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))][col].count()
                total_outliers += outliers
            
            print_kv("Outliers detectados (IQR)", f"{total_outliers} valores ({total_outliers/len(df)*100:.1f}%)")
            
            # An√°lisis de calidad si est√° disponible
            if quality_col:
                print_kv("Variable de calidad identificada", quality_col)
                print_kv("Categor√≠as de calidad", list(df[quality_col].unique()))

    except FileNotFoundError:
        print("‚ö†Ô∏è Archivo no encontrado en la ruta especificada")
        
        show_subsection("Especificaciones del conjunto de datos (referencia)", emoji="üìã")
        show_list([
            "Variables t√≠picas: pH, Temperatura, Sabor, Olor, Grasa, Turbidez",
            "Rango pH: 3.0-9.5 (acidez de la leche)",
            "Temperatura: 34-90¬∞C (procesamiento)",
            "Contenido graso: 0-7% (diferentes tipos de leche)",
        ])
    
    except Exception as e:
        print(f"‚ö†Ô∏è Error al procesar el dataset: {str(e)}")
    
    # ===== Potencial para ML No Supervisado ===== #
    show_subsection("Potencial para ML no supervisado", emoji="üéØ")
    show_list([
        "K-Means para categorizaci√≥n autom√°tica de calidad (Alta/Media/Baja)",
        "Isolation Forest para detecci√≥n de contaminaci√≥n bacteriana",
        "DBSCAN para identificar lotes con caracter√≠sticas excepcionales",
        "PCA para identificar los 2-3 factores m√°s cr√≠ticos de calidad",
        "Gaussian Mixture Models para modelar distribuci√≥n natural de calidad"
    ])

    show_subsection("T√©cnicas recomendadas", emoji="üî¨")
    show_list([
        "Normalizaci√≥n Z-score antes de clustering por diferencias de escala",
        "PCA para reducir dimensionalidad de par√°metros fisicoqu√≠micos",
        "K-Means con elbow method para determinar clusters √≥ptimos",
        "An√°lisis de componentes independientes (ICA) para factores latentes"
    ])
    
    show_subsection("Aplicaci√≥n en entornos productivos", emoji="üè≠")
    show_list([
        "Sistema de clasificaci√≥n autom√°tica en l√≠nea de producci√≥n",
        "Alertas tempranas de desviaciones en calidad",
        "Optimizaci√≥n de par√°metros de procesamiento",
        "Trazabilidad y control de calidad predictivo"
    ], bullet="‚Ä¢")
    
    print()
    print("‚úÖ An√°lisis del Milk Quality Dataset completado exitosamente")


def evaluate_datasets_comparative():
    """
    Funci√≥n para evaluaci√≥n comparativa de los 10 datasets del proyecto.
    
    Eval√∫a cada dataset seg√∫n criterios espec√≠ficos para el contexto del sur de Chile:
    - Diversidad de Datos: Variedad en tipos de datos y estructura
    - Potencial Clustering: Capacidad para generar agrupamientos significativos  
    - Relevancia Impacto: Aplicabilidad en el contexto rural del sur de Chile
    - Disponibilidad: Acceso real a los datos para el proyecto
    - Complementariedad: Sinergia con otros datasets del ecosistema rural
    
    Los puntajes se asignan en escala 1-10 considerando el objetivo principal:
    desarrollo de inteligencia para econom√≠a circular en bosques comestibles,
    monitoreo ganadero y control de calidad l√°ctea en el sur de Chile.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    
    # Criterios de evaluaci√≥n contextualizados al sur de Chile
    # Puntajes basados en discusiones del equipo considerando:
    # - Aplicabilidad en contexto rural sur de Chile
    # - Integraci√≥n con sistemas de econom√≠a circular  
    # - Potencial para inteligencia distribuida con sensores IoT
    # - Relevancia para productores lecheros y forestales
    evaluacion_datasets = {
        'No.': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
        'Dataset': [
            'Dairy Sales',           # 1
            'Animal Sounds',         # 2  
            'Customer Personality',  # 3
            'News Category',         # 4
            'Coffee Health',         # 5
            'Credit Card',           # 6
            'Sleep Health',          # 7
            'Wholesale Customers',   # 8
            'PlantVillage',         # 9
            'Milk Quality'           # 10
        ],
        # Diversidad_Datos: Variedad en estructura y tipos de datos (1-10)
        'Diversidad_Datos': [6, 9, 8, 8, 7, 7, 6, 6, 9, 8],
        
        # Potencial_Clustering: Capacidad de generar clusters significativos (1-10)
        'Potencial_Clustering': [7, 9, 8, 6, 7, 8, 7, 8, 8, 8],
        
        # Relevancia_Impacto: Aplicabilidad directa en el sur de Chile (1-10)
        # Priorizamos: ganader√≠a lechera, agricultura, bienestar animal
        'Relevancia_Impacto': [6, 9, 7, 4, 6, 5, 6, 6, 9, 9],
        
        # Disponibilidad: Acceso garantizado a datos (1-10)
        'Disponibilidad': [5, 9, 5, 5, 5, 5, 5, 5, 9, 10],
        
        # Complementariedad: Sinergia con ecosistema rural inteligente (1-10)
        'Complementariedad': [6, 9, 6, 4, 5, 5, 6, 6, 9, 9]
    }

    df_eval = pd.DataFrame(evaluacion_datasets)

    # Calcular puntuaci√≥n total ponderada
    # Priorizamos relevancia e impacto para el contexto del sur de Chile
    df_eval['Puntuacion_Total'] = (
        df_eval['Diversidad_Datos'] * 0.15 +
        df_eval['Potencial_Clustering'] * 0.20 +
        df_eval['Relevancia_Impacto'] * 0.35 +  # Mayor peso por contexto rural
        df_eval['Disponibilidad'] * 0.15 +
        df_eval['Complementariedad'] * 0.15
    )

    # Ordenar por puntuaci√≥n total
    df_eval = df_eval.sort_values('Puntuacion_Total', ascending=False)

    show_section("Evaluaci√≥n Comparativa de Datasets", emoji="üìä")
    
    # Mostrar contexto de evaluaci√≥n
    print("üéØ Criterios de Evaluaci√≥n para Inteligencia Rural del Sur de Chile:")
    print("   ‚Ä¢ Diversidad de Datos (15%): Variedad en estructura y tipos")
    print("   ‚Ä¢ Potencial Clustering (20%): Capacidad de agrupamientos significativos") 
    print("   ‚Ä¢ Relevancia e Impacto (35%): Aplicabilidad directa en contexto rural")
    print("   ‚Ä¢ Disponibilidad (15%): Acceso garantizado a los datos")
    print("   ‚Ä¢ Complementariedad (15%): Sinergia con ecosistema inteligente")
    print()
    
    # Mostrar tabla de evaluaci√≥n
    display(df_eval.round(2))

    # Crear visualizaciones
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. Ranking por puntuaci√≥n total
    colors = plt.cm.viridis(df_eval['Puntuacion_Total']/df_eval['Puntuacion_Total'].max())
    ax1.barh(df_eval['Dataset'], df_eval['Puntuacion_Total'], color=colors)
    ax1.set_xlabel('Puntuaci√≥n Total')
    ax1.set_title('üèÜ Ranking de Datasets - Contexto Sur de Chile')
    ax1.grid(True, alpha=0.3)
    
    # Anotar puntajes
    for i, (dataset, score) in enumerate(zip(df_eval['Dataset'], df_eval['Puntuacion_Total'])):
        ax1.text(score + 0.1, i, f'{score:.1f}', va='center', ha='left', fontweight='bold')

    # 2. Comparaci√≥n por criterios (Top 5)
    top_5 = df_eval.head(5)
    criterios = ['Diversidad_Datos', 'Potencial_Clustering', 'Relevancia_Impacto', 
                'Disponibilidad', 'Complementariedad']
    
    x = range(len(top_5))
    width = 0.15
    
    for i, criterio in enumerate(criterios):
        ax2.bar([pos + i*width for pos in x], top_5[criterio], 
                width, label=criterio.replace('_', ' '), alpha=0.8)
    
    ax2.set_xlabel('Datasets (Top 5)')
    ax2.set_ylabel('Puntaje (1-10)')
    ax2.set_title('üìã Comparaci√≥n por Criterios - Top 5 Datasets')
    ax2.set_xticks([pos + width*2 for pos in x])
    ax2.set_xticklabels(top_5['Dataset'], rotation=45)
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax2.grid(True, alpha=0.3)

    # 3. Distribuci√≥n por dominio (contextualizada)
    dominios_contexto = {
        'Agropecuario': ['Animal Sounds', 'PlantVillage', 'Milk Quality'],  # Relevantes
        'Comercial': ['Dairy Sales', 'Customer Personality', 'Wholesale Customers'],
        'Salud/Bienestar': ['Coffee Health', 'Sleep Health'], 
        'Otros': ['News Category', 'Credit Card']
    }
    
    conteos_contexto = [len(datasets) for datasets in dominios_contexto.values()]
    colores_contexto = ['#2E8B57', '#4682B4', '#DAA520', '#CD5C5C']  # Verde para agropecuario
    
    wedges, texts, autotexts = ax3.pie(conteos_contexto, labels=list(dominios_contexto.keys()), 
                                      autopct='%1.1f%%', startangle=90, colors=colores_contexto)
    ax3.set_title('üå± Distribuci√≥n por Relevancia en Contexto Rural')
    
    # Destacar sector agropecuario
    wedges[0].set_linewidth(3)
    wedges[0].set_edgecolor('darkgreen')

    # 4. Matriz de correlaci√≥n entre criterios
    criterios_df = df_eval[criterios].corr()
    im = ax4.imshow(criterios_df, cmap='coolwarm', vmin=-1, vmax=1)
    ax4.set_xticks(range(len(criterios)))
    ax4.set_yticks(range(len(criterios)))
    ax4.set_xticklabels([c.replace('_', '\n') for c in criterios], rotation=45)
    ax4.set_yticklabels([c.replace('_', '\n') for c in criterios])
    ax4.set_title('üîó Correlaci√≥n entre Criterios de Evaluaci√≥n')
    
    # A√±adir valores de correlaci√≥n
    for i in range(len(criterios)):
        for j in range(len(criterios)):
            ax4.text(j, i, f'{criterios_df.iloc[i, j]:.2f}', 
                    ha='center', va='center', fontweight='bold')
    
    plt.colorbar(im, ax=ax4, shrink=0.6)

    plt.tight_layout()
    plt.show()

    # Mostrar resultados contextualizados
    show_subsection("üéØ Top 3 Datasets para Inteligencia Rural", emoji="üèÜ")
    top_3 = df_eval.head(3)
    
    for i, (idx, row) in enumerate(top_3.iterrows(), 1):
        relevancia = "üåü ALTA RELEVANCIA" if row['Relevancia_Impacto'] >= 8 else "‚ö° RELEVANCIA MEDIA" if row['Relevancia_Impacto'] >= 6 else "üìã RELEVANCIA B√ÅSICA"
        print(f"{i}. {row['Dataset']} - Puntuaci√≥n: {row['Puntuacion_Total']:.2f} - {relevancia}")

    show_subsection("üí° Justificaci√≥n Estrat√©gica para el Sur de Chile", emoji="üå±")
    show_list([
        "üéµ Animal Sounds: Monitoreo bienestar animal y detecci√≥n autom√°tica de alertas",
        "ü•õ Milk Quality: Control de calidad automatizado en l√≠neas de producci√≥n l√°ctea", 
        "üåø PlantVillage: Diagn√≥stico temprano de enfermedades en cultivos y bosques comestibles",
        "üì° Complementariedad IoT: Integraci√≥n natural con sensores distribuidos",
        "üîÑ Econom√≠a Circular: Optimizaci√≥n de recursos en sistemas agropecuarios sustentables"
    ])
    
    show_subsection("üéØ Alineaci√≥n con Objetivos del Proyecto", emoji="üìä") 
    print("‚Ä¢ Diversidad t√©cnica m√°xima: Audio (se√±ales) + Num√©rico (fisicoqu√≠mico) + Visual (RGB)")
    print("‚Ä¢ Aplicabilidad directa en ganader√≠a lechera del sur de Chile")
    print("‚Ä¢ Potencial para integraci√≥n con rob√≥tica e IoT distribuido")
    print("‚Ä¢ Base para inteligencia de econom√≠a circular en entornos rurales")
    
    return df_eval


def generate_datasets_summary_table():
    """
    Genera y muestra la tabla resumen de los 10 datasets con informaci√≥n completa
    incluyendo descripci√≥n, dominio, tipo de datos y aplicaciones en ML no supervisado.
    """
    datasets_info = {
        "No.": list(range(1, 11)),
        "Dataset": [
            "Dairy Goods Sales Dataset", "Animal Sounds Dataset",
            "Customer Personality Analysis", "News Category Dataset",
            "Global Coffee Health Dataset", "Credit Card Customer",
            "Sleep Health and Lifestyle", "Wholesale Customers Dataset",
            "PlantVillage Dataset", "Milk Quality Dataset"
        ],
        "Dominio": [
            "Comercio/Ventas", "Audio/Sonidos", "Comportamiento del Consumidor",
            "Texto/NLP", "Salud Global", "Servicios Financieros",
            "Salud y Bienestar", "Comercio B2B", "Visi√≥n por Computador",
            "Calidad Alimentaria"
        ],
        "Tipo_de_Datos": [
            "Transaccional", "Audio (WAV)", "Demogr√°fico/Comportamental",
            "Texto/JSON", "Num√©rico/Geogr√°fico", "Financiero/Num√©rico",
            "M√©dico/Estilo de vida", "Ventas por categor√≠a",
            "Im√°genes RGB", "Fisicoqu√≠mico/Num√©rico"
        ],
        "Formato_Archivo": [
            "CSV", "WAV/Audio", "CSV", "JSON", "CSV",
            "CSV", "CSV", "CSV", "JPG/PNG", "CSV"
        ],
        "URL_Origen": [
            "https://www.kaggle.com/datasets/suraj520/dairy-goods-sales-dataset",
            "https://github.com/YashNita/Animal-Sound-Dataset",
            "https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis",
            "https://www.kaggle.com/datasets/rmisra/news-category-dataset",
            "https://www.kaggle.com/datasets/uom190346a/global-coffee-health-dataset",
            "https://www.kaggle.com/datasets/sakshigoyal7/credit-card-customers",
            "https://www.kaggle.com/datasets/uom190346a/sleep-health-and-lifestyle-dataset",
            "https://www.kaggle.com/datasets/binovi/wholesale-customers-data-set",
            "https://www.kaggle.com/datasets/plantvillage-dataset",
            "https://www.kaggle.com/datasets/cpluzshrijayan/milkquality"
        ],
        "Aplicaciones_ML_No_Supervisado": [
            "Segmentaci√≥n clientes, Patrones compra",
            "Clustering sonidos, Clasificaci√≥n no supervisada",
            "Segmentaci√≥n personalidad, Perfiles cliente",
            "Topic modeling, Clustering temas",
            "Clustering geogr√°fico, Patrones consumo",
            "Detecci√≥n fraudes, Clustering crediticio",
            "Patrones sue√±o, Clustering estilos vida",
            "Segmentaci√≥n B2B, Patrones compra",
            "Clustering visual, Detecci√≥n anomal√≠as",
            "Control calidad, Detecci√≥n anomal√≠as"
        ]
    }

    df_summary = pd.DataFrame(datasets_info)

    # üíÖ Estilo visual con pandas Styler
    styled_df = (
        df_summary.style
        .set_table_styles([
            {"selector": "thead th", "props": [("background-color", "#4C72B0"), ("color", "white"), ("font-weight", "bold")]},
            {"selector": "tbody tr:nth-child(even)", "props": [("background-color", "#f2f2f2")]},
            {"selector": "tbody tr:nth-child(odd)", "props": [("background-color", "white")]}
        ])
        .set_properties(**{
            "text-align": "left",
            "border": "1px solid #ddd",
            "color": "#222222"   # üîπ color de letra m√°s oscuro dentro de las celdas
        })
        .hide(axis="index")
    )

    show_section("Tabla resumen de los 10 datasets", emoji="üìã")
    display(styled_df)

    # üßæ Resumen general
    blank()
    print_kv("Total de datasets analizados", len(df_summary))
    print_kv("Dominios cubiertos", df_summary["Dominio"].nunique())
    print_kv("Tipos de formato", ", ".join(df_summary["Formato_Archivo"].unique()))
    
    return df_summary
